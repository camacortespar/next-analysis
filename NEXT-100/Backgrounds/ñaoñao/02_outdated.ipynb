{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productions @ Event-level\n",
    "\n",
    "Ultimately, our objective is to conduct a comparative analysis of the data and simulations.\n",
    "<br>\n",
    "Therefore, it is important to have both information with the same structure at event-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/lustre/ific.uv.es/prj/gl/neutrinos/users/ccortesp/libs')\n",
    "\n",
    "import crudo\n",
    "import fit_functions as ff\n",
    "import glob\n",
    "from   iminuit import Minuit\n",
    "from   iminuit.cost import LeastSquares\n",
    "from   invisible_cities.reco.corrections import read_maps\n",
    "from   invisible_cities.reco.corrections import apply_all_correction\n",
    "from   invisible_cities.types.symbols    import NormStrategy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotting_tools as pt\n",
    "import tables as tb\n",
    "from   scipy import integrate\n",
    "from   scipy.optimize import curve_fit\n",
    "from   scipy.stats    import linregress\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Important Directories ----- #\n",
    "data_dir   = '/lustre/ific.uv.es/prj/gl/neutrinos/users/ccortesp/NEXT-100/sophronia/'                           # Runs per ldc\n",
    "icaros_dir = '/lustre/ific.uv.es/prj/gl/neutrinos/users/ccortesp/NEXT-100/icaros/'                              # Kr maps per run\n",
    "MC_dir     = '/lustre/ific.uv.es/prj/gl/neutrinos/NEXT/MC/NEXT100/Radiogenics/LPR/IC_v2.3.1/NEXUS_v7_08_01/'    # MC directory\n",
    "output_dir = '/lustre/ific.uv.es/prj/gl/neutrinos/users/ccortesp/NEXT-100/Backgrounds/h5/'                      # Output directory for the h5 files\n",
    "\n",
    "# Paths of the tables inside the HDF5 file. These are the keys used to access the datasets\n",
    "dorothea_key  = '/DST/Events'\n",
    "sophronia_key = '/RECO/Events'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the low-background runs\n",
    "runs_info = {\n",
    "                # Condition: castle = closed & RAS = on #\n",
    "                # Jun 2025\n",
    "                15502: {\"duration\": 85477, \"OK\": 28287, \"LOST\": 9564  , \"proc_eff\": 1.0   },\n",
    "                # 15504: {\"duration\": 85194, \"OK\": 28297, \"LOST\": 9537  , \"proc_eff\": 0.9999},\n",
    "                # 15505: {\"duration\": 86517, \"OK\": 28632, \"LOST\": 9725  , \"proc_eff\": 0.9999},\n",
    "                # 15506: {\"duration\": 84841, \"OK\": 28438, \"LOST\": 9603  , \"proc_eff\": 0.9999},\n",
    "                # 15507: {\"duration\": 55740, \"OK\": 18569, \"LOST\": 6149  , \"proc_eff\": 0.9999},\n",
    "                # 15514: {\"duration\": 59207, \"OK\": 20054, \"LOST\": 6646  , \"proc_eff\": 0.9999},\n",
    "                # 15519: {\"duration\": 34045, \"OK\": 11420, \"LOST\": 3646  , \"proc_eff\": 0.9998},\n",
    "                # 15520: {\"duration\": 85170, \"OK\": 28050, \"LOST\": 9130  , \"proc_eff\": 1.0   },\n",
    "                # 15521: {\"duration\": 85388, \"OK\": 28396, \"LOST\": 8831  , \"proc_eff\": 1.0   },\n",
    "                # 15527: {\"duration\": 69725, \"OK\": 23560, \"LOST\": 7411  , \"proc_eff\": 1.0   },\n",
    "                # 15528: {\"duration\": 41361, \"OK\": 13460, \"LOST\": 4426  , \"proc_eff\": 1.0   },\n",
    "                # 15535: {\"duration\": 84904, \"OK\": 28799, \"LOST\": 9156  , \"proc_eff\": 0.9998},\n",
    "                # 15539: {\"duration\": 56567, \"OK\": 15618, \"LOST\": 9586  , \"proc_eff\": 0.9997},\n",
    "                # 15540: {\"duration\": 67663, \"OK\": 22526, \"LOST\": 7066  , \"proc_eff\": 0.9993},\n",
    "                # 15541: {\"duration\": 86630, \"OK\": 29124, \"LOST\": 9333  , \"proc_eff\": 1.0   },\n",
    "                # 15542: {\"duration\": 87915, \"OK\": 29717, \"LOST\": 9274  , \"proc_eff\": 1.0   },\n",
    "                # 15543: {\"duration\": 86570, \"OK\": 29160, \"LOST\": 9123  , \"proc_eff\": 1.0   },\n",
    "                # 15544: {\"duration\": 86566, \"OK\": 29498, \"LOST\": 9029  , \"proc_eff\": 1.0   },\n",
    "                # 15545: {\"duration\": 85892, \"OK\": 29437, \"LOST\": 8877  , \"proc_eff\": 1.0   },\n",
    "                # 15546: {\"duration\": 84822, \"OK\": 28663, \"LOST\": 8704  , \"proc_eff\": 1.0   },\n",
    "                # 15547: {\"duration\": 71594, \"OK\": 24549, \"LOST\": 7421  , \"proc_eff\": 1.0   },\n",
    "                # 15557: {\"duration\": 66088, \"OK\": 22273, \"LOST\": 6929  , \"proc_eff\": 1.0   },\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global\n",
    "V_drift = 0.865     # in [mm/μs]\n",
    "\n",
    "# Setting up the filter of spurious hits\n",
    "filter_config = {'distance': [15., 15.], 'nhit': 3, 'variables': ['E_corr']}\n",
    "NO_hits_sp = crudo.drop_isolated_clusters(**filter_config)\n",
    "\n",
    "# ----- Information for S1e Correction ----- #\n",
    "# Cathode temporal position\n",
    "DT_stop = 1372.2543             # in [μs]\n",
    "# Fit values\n",
    "CV_fit = [0.62, 869.87]         # [slope, intercept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing run 15502 ---\n",
      "  → 24332 reconstructed events\n",
      "\n",
      "--- Processing run 15504 ---\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.29 TiB for an array with shape (421125, 421125) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Drop isolated clusters of hits (less than 4)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m data_event \u001b[38;5;241m=\u001b[39m df_sophronia\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m df_sophronia \u001b[38;5;241m=\u001b[39m \u001b[43mdata_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNO_hits_sp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m df_sophronia \u001b[38;5;241m=\u001b[39m df_sophronia\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# ----- Aggregated Sophronia Data ----- #\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Aggregation functions\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/ific.uv.es/prj/gl/neutrinos/NEXT/sw/miniconda/envs/IC-3.8-2022-04-13/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1275\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m group_selection_context(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m/lustre/ific.uv.es/prj/gl/neutrinos/NEXT/sw/miniconda/envs/IC-3.8-2022-04-13/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1309\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28mself\u001b[39m, f: F, data: FrameOrSeriesUnion\n\u001b[1;32m   1293\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1309\u001b[0m     keys, values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_applied_output(\n\u001b[1;32m   1312\u001b[0m         data, keys, values, not_indexed_same\u001b[38;5;241m=\u001b[39mmutated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutated\n\u001b[1;32m   1313\u001b[0m     )\n",
      "File \u001b[0;32m/lustre/ific.uv.es/prj/gl/neutrinos/NEXT/sw/miniconda/envs/IC-3.8-2022-04-13/lib/python3.8/site-packages/pandas/core/groupby/ops.py:804\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m     sdata \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msorted_data\n\u001b[0;32m--> 804\u001b[0m     result_values, mutated \u001b[38;5;241m=\u001b[39m \u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# This is a rare case in which re-running in python-space may\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m#  make a difference, see  test_apply_mutate.test_mutate_groups\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/ific.uv.es/prj/gl/neutrinos/NEXT/sw/miniconda/envs/IC-3.8-2022-04-13/lib/python3.8/site-packages/pandas/core/groupby/ops.py:1349\u001b[0m, in \u001b[0;36mFrameSplitter.fast_apply\u001b[0;34m(self, f, sdata, names)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfast_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: F, sdata: FrameOrSeries, names):\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;66;03m# must return keys::list, values::list, mutated::bool\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m     starts, ends \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mgenerate_slices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslabels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups)\n\u001b[0;32m-> 1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_frame_axis0\u001b[49m\u001b[43m(\u001b[49m\u001b[43msdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/ific.uv.es/prj/gl/neutrinos/NEXT/sw/miniconda/envs/IC-3.8-2022-04-13/lib/python3.8/site-packages/pandas/_libs/reduction.pyx:381\u001b[0m, in \u001b[0;36mpandas._libs.reduction.apply_frame_axis0\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/lustre/ific.uv.es/prj/gl/neutrinos/users/ccortesp/libs/crudo.py:315\u001b[0m, in \u001b[0;36mdrop_isolated_clusters.<locals>.drop_isolated_clusters\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    313\u001b[0m y       \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mY\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    314\u001b[0m xy      \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((x,y))\n\u001b[0;32m--> 315\u001b[0m dr2     \u001b[38;5;241m=\u001b[39m \u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxy\u001b[49m\u001b[43m)\u001b[49m                 \u001b[38;5;66;03m# Compute the distance between all hits\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(dr2\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m0\u001b[39m]                  \u001b[38;5;66;03m# Empty dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/spatial/distance.py:2939\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2938\u001b[0m     cdist_fn \u001b[38;5;241m=\u001b[39m metric_info\u001b[38;5;241m.\u001b[39mcdist_func\n\u001b[0;32m-> 2939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2941\u001b[0m     metric_info \u001b[38;5;241m=\u001b[39m _TEST_METRICS\u001b[38;5;241m.\u001b[39mget(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.29 TiB for an array with shape (421125, 421125) and data type float64"
     ]
    }
   ],
   "source": [
    "# Efficiency counters\n",
    "sophronia_events  = 0\n",
    "Znegative_events  = 0\n",
    "final_reco_events = 0\n",
    "\n",
    "# Store all the processed dfs\n",
    "all_processed_dfs = []\n",
    "\n",
    "# ----- Run Loop ----- #\n",
    "for run_id, entry in runs_info.items():\n",
    "\n",
    "    print(f\"--- Processing run {run_id} ---\")\n",
    "\n",
    "    # ----- Krypton Map ----- #\n",
    "    try:\n",
    "        kr_file = f'run_{run_id}.v2.3.1.20250512.Kr.map.h5'\n",
    "        kr_path = os.path.join(icaros_dir, kr_file)\n",
    "        # Read map and create correction function\n",
    "        cmap = read_maps(kr_path)\n",
    "        corr_func = apply_all_correction(cmap, apply_temp=False, norm_strat=NormStrategy.kr)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Kr map for run {run_id} not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Run counter\n",
    "    reco_evts = 0\n",
    "    events = 0\n",
    "\n",
    "    # ----- LDC Loop ----- #\n",
    "    for ldc in range(1, 8):\n",
    "\n",
    "        # Load the HDF5 file\n",
    "        h5_path = os.path.join(data_dir, f'run_{run_id}_ldc{ldc}_trg2_sophronia.h5')\n",
    "\n",
    "        # ----- Sophronia ----- #\n",
    "        df_sophronia = pd.DataFrame()\n",
    "        try:\n",
    "            df_sophronia = pd.read_hdf(h5_path, key=sophronia_key, columns=['event', 'time', 'npeak', 'X', 'Y', 'Z', 'E'])\n",
    "            print(f\"  After reco chain: {df.sophronia.event.nunique()} events\")\n",
    "            sophronia_events += df_sophronia.event.nunique()\n",
    "        except KeyError:\n",
    "            print(f\"Key {sophronia_key} not found in {h5_path}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # ----- Energy Correction ----- #\n",
    "        df_sophronia['corr_factor'] = corr_func(df_sophronia.X, df_sophronia.Y, df_sophronia.Z, df_sophronia.time)      # In data, DT = Z\n",
    "        df_sophronia['E_corr'] = df_sophronia['E'] * df_sophronia['corr_factor']\n",
    "\n",
    "        # Set hits with NaN or negative energy to 0\n",
    "        df_sophronia['E_corr'] = np.where(\n",
    "                                            pd.notna(df_sophronia['E_corr']) & (df_sophronia['E_corr'] > 0),        # Condition\n",
    "                                            df_sophronia['E_corr'],                                                 # Value if condition is True\n",
    "                                            0                                                                       # Value if condition is False   \n",
    "        )\n",
    "\n",
    "        # Drop events with negative Z\n",
    "        bad_evt_ids = df_sophronia.loc[df_sophronia['Z'] < 0, 'event'].unique()\n",
    "        Znegative_events += len(bad_evt_ids)\n",
    "\n",
    "        if bad_evt_ids.size > 0:\n",
    "            df_sophronia = df_sophronia[~df_sophronia['event'].isin(bad_evt_ids)].copy()\n",
    "\n",
    "        # Compute real Z position: using the drift velocity\n",
    "        df_sophronia['Z_real'] = df_sophronia['Z'] * V_drift\n",
    "\n",
    "        # Drop isolated clusters of hits (less than 4)\n",
    "        data_event = df_sophronia.groupby('event')\n",
    "        df_sophronia = data_event.apply(NO_hits_sp)\n",
    "        df_sophronia = df_sophronia.reset_index(drop=True)\n",
    "\n",
    "        # ----- Aggregated Sophronia Data ----- #\n",
    "        # Aggregation functions\n",
    "        def weighted_avg(series, weight):\n",
    "            if weight.sum() == 0:                           # Avoid division by zero\n",
    "                return np.nan\n",
    "            return np.average(series, weights=weight)\n",
    "\n",
    "        def R_max_func(group_df):\n",
    "            return np.sqrt(group_df['X']**2 + group_df['Y']**2).max()\n",
    "        \n",
    "        # Relevant information from Sophronia: at event and npeak level\n",
    "        df_file = df_sophronia.groupby(['event', 'npeak'], as_index=False).agg(\n",
    "\n",
    "                        # Weighted averages for X, Y, Z\n",
    "                        X=('X',      lambda x: weighted_avg(x, df_sophronia.loc[x.index, 'E_corr'])),\n",
    "                        Y=('Y',      lambda y: weighted_avg(y, df_sophronia.loc[y.index, 'E_corr'])),\n",
    "                        Z=('Z_real', lambda z: weighted_avg(z, df_sophronia.loc[z.index, 'E_corr'])),\n",
    "                        # Sum of Ec\n",
    "                        Ec=('E_corr', 'sum'),\n",
    "                        # Min and max of Z\n",
    "                        Z_min=('Z_real', 'min'),\n",
    "                        Z_max=('Z_real', 'max'),\n",
    "                        # Max R\n",
    "                        # For R_max, the lambda needs the group DataFrame to access both X and Y\n",
    "                        R_max=('X', lambda xy_group: R_max_func(df_sophronia.loc[xy_group.index]))\n",
    "\n",
    "        )\n",
    "\n",
    "        # ----- Dorothea ----- #\n",
    "        df_dorothea = pd.DataFrame()\n",
    "        try:\n",
    "            df_dorothea = pd.read_hdf(h5_path, key=dorothea_key, columns=['event', 'nS1', 'nS2', 'S1h', 'S1e', 'DT'])\n",
    "        except KeyError:\n",
    "            print(f\"Key {dorothea_key} not found in {h5_path}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # MATAR PO-LIKES PRIMERO\n",
    "\n",
    "        # ----- S1e Correction ----- #\n",
    "        df_dorothea = crudo.correct_S1e(df_dorothea, CV_fit, DT_stop, output_column='S1e_corr')\n",
    "\n",
    "        # Relevant information from Dorothea: at event level\n",
    "        dorothea_agg = df_dorothea.groupby('event', as_index=False).agg(\n",
    "\n",
    "                            nS1=('nS1', 'max'),\n",
    "                            nS2=('nS2', 'max'),\n",
    "                            S1h=('S1h', 'max'),               # For events with > 1 S1, assign the max S1h\n",
    "                            S1e_corr=('S1e_corr', 'max')      # Same here\n",
    "\n",
    "        )\n",
    "\n",
    "        # Merge aggregated Dorothea data with Sophronia data\n",
    "        df_file = pd.merge(df_file, dorothea_agg, on='event', how='left')\n",
    "\n",
    "        # ----- Run Information ----- #\n",
    "        df_file['Run'] = run_id\n",
    "\n",
    "        # Append to the main DataFrame\n",
    "        all_processed_dfs.append(df_file)\n",
    "        \n",
    "        events += df_file.event.nunique()                # Per run\n",
    "        final_reco_events += df_file.event.nunique()     # Total\n",
    "\n",
    "    print(f\"  → {events} reconstructed events\\n\")\n",
    "\n",
    "print(f\"\\nTotal Sophronia events processed     = {sophronia_events}\")\n",
    "print(f\"Total events with negative Z dropped = {Znegative_events}\")\n",
    "print(f\"Total final events after processing  = {final_reco_events}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y ya, eso es todo, eso es todo\n"
     ]
    }
   ],
   "source": [
    "# Concatenation\n",
    "if all_processed_dfs:\n",
    "    Data_df = pd.concat(all_processed_dfs, ignore_index=True)\n",
    "\n",
    "print('Y ya, eso es todo, eso es todo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Let's store the final dataframe as an HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /lustre/ific.uv.es/prj/gl/neutrinos/users/ccortesp/NEXT-100/Backgrounds/h5/Data_event_background.h5\n"
     ]
    }
   ],
   "source": [
    "# Output filename\n",
    "h5_name = 'Data_event_background.h5'\n",
    "h5_path = os.path.join(output_dir, h5_name)\n",
    "\n",
    "# Store it!\n",
    "Data_df.to_hdf(h5_path, key='/Data/Events', mode='w', format='table')\n",
    "print(f\"Data saved to {h5_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or... if you want to merge two different HDF5 files, you can do it ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of the HDF5 files\n",
    "temp_h5_path = os.path.join(output_dir, 'temp.h5')\n",
    "temp_df = pd.read_hdf(temp_h5_path, key='/Data/Events')\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = pd.concat([Data_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Output filename for the merged file\n",
    "merged_h5_name = 'Merged_Data_event_background.h5'\n",
    "merged_h5_path = os.path.join(output_dir, merged_h5_name)\n",
    "\n",
    "# Store the merged DataFrame\n",
    "merged_df.to_hdf(merged_h5_path, key='/Data/Events', mode='w', format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Production\n",
    "\n",
    "First, write down all the basic information about the simulation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Isotopes ----- #\n",
    "Isotopes = ['Bi214', 'Tl208', 'Co60', 'K40']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Bi214 ----\n",
      "\n",
      "Processing information in 23 volumes...\n",
      "  OPTICAL_PAD volume: 24 files\n",
      "  → 22239 reco events processed\n",
      "  GATE_RING volume: 3 files\n",
      "  → 4661 reco events processed\n",
      "  LIGHT_TUBE volume: 11 files\n",
      "  → 51637 reco events processed\n",
      "  PMT volume: 66 files\n",
      "  → 45168 reco events processed\n",
      "  EDPM_SEAL volume: 156 files\n",
      "  → 1 reco events processed\n",
      "  PEDESTAL volume: 1000 files\n",
      "  → 611 reco events processed\n",
      "  EP_COPPER_PLATE volume: 258 files\n",
      "  → 35737 reco events processed\n",
      "  DB_PLUG volume: 489 files\n",
      "  → 1761 reco events processed\n",
      "  TP_COPPER_PLATE volume: 7 files\n",
      "  → 1403 reco events processed\n",
      "  FIELD_RING volume: 88 files\n",
      "  → 168820 reco events processed\n",
      "  SAPPHIRE_WINDOW volume: 208 files\n",
      "  → 198686 reco events processed\n",
      "  SHIELDING_STEEL volume: 64 files\n",
      "  → 196 reco events processed\n",
      "  SHIELDING_STRUCT volume: 1000 files\n",
      "  → 308 reco events processed\n",
      "  BUBBLE_SEAL volume: 1000 files\n",
      "  → 1 reco events processed\n",
      "  CATHODE_RING volume: 3 files\n",
      "  → 5541 reco events processed\n",
      "  ANODE_RING volume: 3 files\n",
      "  → 4109 reco events processed\n",
      "  HDPE_TUBE volume: 9 files\n",
      "  → 13637 reco events processed\n",
      "  VESSEL volume: 1000 files\n",
      "  → 17678 reco events processed\n",
      "  SIPM_BOARD volume: 112 files\n",
      "  → 210815 reco events processed\n",
      "  ICS volume: 167 files\n",
      "  → 38964 reco events processed\n",
      "  SHIELDING_LEAD volume: 1000 files\n",
      "  → 2852 reco events processed\n",
      "  PMT_BASE volume: 129 files\n",
      "  → 22414 reco events processed\n",
      "\n",
      "--- Processing Tl208 ----\n",
      "\n",
      "Processing information in 22 volumes...\n",
      "  OPTICAL_PAD volume: 10 files\n",
      "  → 16258 reco events processed\n",
      "  GATE_RING volume: 1 files\n",
      "  → 352 reco events processed\n",
      "  LIGHT_TUBE volume: 25 files\n",
      "  → 144754 reco events processed\n",
      "  PMT volume: 35 files\n",
      "  → 43775 reco events processed\n",
      "  EDPM_SEAL volume: 39 files\n",
      "  → 0 reco events processed\n",
      "  PEDESTAL volume: 1000 files\n",
      "  → 2493 reco events processed\n",
      "  EP_COPPER_PLATE volume: 149 files\n",
      "  → 40577 reco events processed\n",
      "  DB_PLUG volume: 372 files\n",
      "  → 5569 reco events processed\n",
      "  TP_COPPER_PLATE volume: 1 files\n",
      "  → 261 reco events processed\n",
      "  FIELD_RING volume: 8 files\n",
      "  → 26223 reco events processed\n",
      "  SAPPHIRE_WINDOW volume: 68 files\n",
      "  → 112990 reco events processed\n",
      "  SHIELDING_STEEL volume: 27 files\n",
      "  → 314 reco events processed\n",
      "  SHIELDING_STRUCT volume: 1000 files\n",
      "  → 1112 reco events processed\n",
      "  BUBBLE_SEAL volume: 1000 files\n",
      "  → 0 reco events processed\n",
      "  CATHODE_RING volume: 1 files\n",
      "  → 424 reco events processed\n",
      "  ANODE_RING volume: 1 files\n",
      "  → 343 reco events processed\n",
      "  HDPE_TUBE volume: 1 files\n",
      "  → 2980 reco events processed\n",
      "  VESSEL volume: 1000 files\n",
      "  → 24080 reco events processed\n",
      "  SIPM_BOARD volume: 29 files\n",
      "  → 89281 reco events processed\n",
      "  ICS volume: 24 files\n",
      "  → 11832 reco events processed\n",
      "  SHIELDING_LEAD volume: 684 files\n",
      "  → 2320 reco events processed\n",
      "  PMT_BASE volume: 48 files\n",
      "  → 15246 reco events processed\n",
      "\n",
      "--- Processing Co60 ----\n",
      "\n",
      "Processing information in 22 volumes...\n",
      "  OPTICAL_PAD volume: 11 files\n",
      "  → 17263 reco events processed\n",
      "  GATE_RING volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  LIGHT_TUBE volume: 1 files\n",
      "  → 2709 reco events processed\n",
      "  PMT volume: 719 files\n",
      "  → 846503 reco events processed\n",
      "  EDPM_SEAL volume: 21 files\n",
      "  → 0 reco events processed\n",
      "  PEDESTAL volume: 1000 files\n",
      "  → 875 reco events processed\n",
      "  EP_COPPER_PLATE volume: 1000 files\n",
      "  → 251129 reco events processed\n",
      "  DB_PLUG volume: 5 files\n",
      "  → 23 reco events processed\n",
      "  TP_COPPER_PLATE volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  FIELD_RING volume: 7 files\n",
      "  → 24066 reco events processed\n",
      "  SAPPHIRE_WINDOW volume: 1 files\n",
      "  → 2 reco events processed\n",
      "  SHIELDING_STEEL volume: 109 files\n",
      "  → 559 reco events processed\n",
      "  SHIELDING_STRUCT volume: 1000 files\n",
      "  → 497 reco events processed\n",
      "  BUBBLE_SEAL volume: 36 files\n",
      "  → 0 reco events processed\n",
      "  CATHODE_RING volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  ANODE_RING volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  HDPE_TUBE volume: 10 files\n",
      "  → 26489 reco events processed\n",
      "  VESSEL volume: 1000 files\n",
      "  → 13955 reco events processed\n",
      "  SIPM_BOARD volume: 19 files\n",
      "  → 60407 reco events processed\n",
      "  ICS volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  SHIELDING_LEAD volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  PMT_BASE volume: 18 files\n",
      "  → 5528 reco events processed\n",
      "\n",
      "--- Processing K40 ----\n",
      "\n",
      "Processing information in 22 volumes...\n",
      "  OPTICAL_PAD volume: 154 files\n",
      "  → 12890 reco events processed\n",
      "  GATE_RING volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  LIGHT_TUBE volume: 24 files\n",
      "  → 34098 reco events processed\n",
      "  PMT volume: 1000 files\n",
      "  → 65193 reco events processed\n",
      "  EDPM_SEAL volume: 407 files\n",
      "  → 0 reco events processed\n",
      "  PEDESTAL volume: 1000 files\n",
      "  → 91 reco events processed\n",
      "  EP_COPPER_PLATE volume: 1000 files\n",
      "  → 14937 reco events processed\n",
      "  DB_PLUG volume: 596 files\n",
      "  → 249 reco events processed\n",
      "  TP_COPPER_PLATE volume: 1 files\n",
      "  → 11 reco events processed\n",
      "  FIELD_RING volume: 56 files\n",
      "  → 9915 reco events processed\n",
      "  SAPPHIRE_WINDOW volume: 39 files\n",
      "  → 3477 reco events processed\n",
      "  SHIELDING_STEEL volume: 602 files\n",
      "  → 228 reco events processed\n",
      "  SHIELDING_STRUCT volume: 1000 files\n",
      "  → 45 reco events processed\n",
      "  BUBBLE_SEAL volume: 1000 files\n",
      "  → 4 reco events processed\n",
      "  CATHODE_RING volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  ANODE_RING volume: 0 files\n",
      "  → 0 reco events processed\n",
      "  HDPE_TUBE volume: 650 files\n",
      "  → 86438 reco events processed\n",
      "  VESSEL volume: 1000 files\n",
      "  → 8709 reco events processed\n",
      "  SIPM_BOARD volume: 536 files\n",
      "  → 88440 reco events processed\n",
      "  ICS volume: 3 files\n",
      "  → 100 reco events processed\n",
      "  SHIELDING_LEAD volume: 1000 files\n",
      "  → 100 reco events processed\n",
      "  PMT_BASE volume: 965 files\n",
      "  → 16822 reco events processed\n"
     ]
    }
   ],
   "source": [
    "# Store all the processed dfs\n",
    "all_processed_dfs = []\n",
    "\n",
    "# ----- Isotopes Loop ----- #\n",
    "for isotope in Isotopes:\n",
    "\n",
    "    print(f\"\\n--- Processing {isotope} ---\\n\")\n",
    "\n",
    "    # Isotope directory and sub-folders (volumes)\n",
    "    isotope_dir     = os.path.join(MC_dir, isotope)\n",
    "    isotope_folders = [folder for folder in os.listdir(isotope_dir) if os.path.isdir(os.path.join(isotope_dir, folder))]\n",
    "    print(f\"Processing information in {len(isotope_folders)} volumes...\")\n",
    "\n",
    "    # ----- Volumes Loop ----- #\n",
    "    for volume in isotope_folders:\n",
    "\n",
    "        # Skip active volume. We do not expect background there.\n",
    "        if volume == 'ACTIVE':\n",
    "            continue\n",
    "\n",
    "        # MC Sophronia path\n",
    "        soph_dir = os.path.join(isotope_dir, volume, 'sophronia')\n",
    "        # List all .h5 files in the MC Sophronia directory in alphabetical order\n",
    "        files = sorted(glob.glob(os.path.join(soph_dir, '*.h5')))\n",
    "        print(f\"  {volume} volume: {len(files)} files\")\n",
    "\n",
    "        # Auxiliary variables\n",
    "        reco_evts = 0\n",
    "\n",
    "        # ----- Files Loop ----- #\n",
    "        for file in files:\n",
    "\n",
    "            # ----- Sophronia ----- #\n",
    "            df_sophronia = pd.DataFrame()\n",
    "            try:\n",
    "                df_sophronia = pd.read_hdf(file, key=sophronia_key, columns=['event', 'npeak', 'X', 'Y', 'Z', 'Ec'])\n",
    "            except KeyError:\n",
    "                print(f\"Key {sophronia_key} not found in {file}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Set hits with NaN or negative energy to 0\n",
    "            df_sophronia['Ec'] = np.where(\n",
    "                                                pd.notna(df_sophronia['Ec']) & (df_sophronia['Ec'] > 0),        # Condition\n",
    "                                                df_sophronia['Ec'],                                             # Value if condition is True\n",
    "                                                0                                                               # Value if condition is False   \n",
    "            )\n",
    "\n",
    "            # Aggregation functions\n",
    "            def weighted_avg(series, weight):\n",
    "                if weight.sum() == 0:                           # Avoid division by zero\n",
    "                    return np.nan\n",
    "                return np.average(series, weights=weight)\n",
    "\n",
    "            def R_max_func(group_df):\n",
    "                return np.sqrt(group_df['X']**2 + group_df['Y']**2).max()\n",
    "            \n",
    "            # Relevant information from Sophronia: at event and npeak level\n",
    "            df_file = df_sophronia.groupby(['event', 'npeak'], as_index=False).agg(\n",
    "\n",
    "                            # Weighted averages for X, Y, Z\n",
    "                            X=('X', lambda x: weighted_avg(x, df_sophronia.loc[x.index, 'Ec'])),\n",
    "                            Y=('Y', lambda y: weighted_avg(y, df_sophronia.loc[y.index, 'Ec'])),\n",
    "                            Z=('Z', lambda z: weighted_avg(z, df_sophronia.loc[z.index, 'Ec'])),\n",
    "                            # Sum of Ec\n",
    "                            Ec=('Ec', 'sum'),\n",
    "                            # Min and max of Z\n",
    "                            Z_min=('Z', 'min'),\n",
    "                            Z_max=('Z', 'max'),\n",
    "                            # Max R\n",
    "                            # For R_max, the lambda needs the group DataFrame to access both X and Y\n",
    "                            R_max=('X', lambda xy_group: R_max_func(df_sophronia.loc[xy_group.index]))\n",
    "\n",
    "            )\n",
    "\n",
    "            # ----- Dorothea ----- #\n",
    "            df_dorothea = pd.DataFrame()\n",
    "            try:\n",
    "                df_dorothea = pd.read_hdf(file, key=dorothea_key, columns=['event', 'nS1', 'nS2', 'S1e'])\n",
    "            except KeyError:\n",
    "                print(f\"Key {dorothea_key} not found in {file}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Relevant information from Dorothea: at event level\n",
    "            dorothea_agg = df_dorothea.groupby('event', as_index=False).agg(\n",
    "\n",
    "                                nS1=('nS1', 'max'),\n",
    "                                nS2=('nS2', 'max'),\n",
    "                                S1e=('S1e', 'max')      # For events with > 1 S1, assign the max S1e\n",
    "\n",
    "            )\n",
    "\n",
    "            # Merge aggregated Dorothea data with Sophronia data\n",
    "            df_file = pd.merge(df_file, dorothea_agg, on='event', how='left')\n",
    "\n",
    "            # ----- Monte Carlo Information ----- #\n",
    "            df_file['Isotope'] = isotope\n",
    "            df_file['Volume']  = volume    \n",
    "\n",
    "            # Append to the main DataFrame\n",
    "            all_processed_dfs.append(df_file)\n",
    "            reco_evts += df_file.event.nunique()\n",
    "\n",
    "        print(f\"  → {reco_evts} reco events processed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y ya, eso es todo, eso es todo\n"
     ]
    }
   ],
   "source": [
    "# Concatenation\n",
    "if all_processed_dfs:\n",
    "    MC_df = pd.concat(all_processed_dfs, ignore_index=True)\n",
    "\n",
    "print('Y ya, eso es todo, eso es todo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Let's store the final dataframe as an HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /lustre/ific.uv.es/prj/gl/neutrinos/users/ccortesp/NEXT-100/Backgrounds/h5/\n"
     ]
    }
   ],
   "source": [
    "# Output filename\n",
    "h5_name = 'MC_event_background.h5'\n",
    "h5_path = os.path.join(output_dir, h5_name)\n",
    "\n",
    "# Store it!\n",
    "MC_df.to_hdf(h5_path, key='/MC/Events', mode='w', format='table')\n",
    "print(f\"Simulation saved to {h5_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
